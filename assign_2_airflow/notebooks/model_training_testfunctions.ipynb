{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcc10dd-ddde-4757-ae60-3886656c1e38",
   "metadata": {},
   "source": [
    "## 1. Testing to see if scripts exist in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83597308-e722-47d4-b096-f43d573d1e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Checking scripts directory...\n",
      "Available scripts:\n",
      "  - model_training_LR.py\n",
      "  - model_training_RF.py\n",
      "  - model_training_XG.py\n",
      "  - processing_bronze_table.py\n",
      "  - processing_gold_table.py\n",
      "  - processing_silver_table.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "SCRIPTS_DIR = \"/app/utils\"\n",
    "\n",
    "print(\"üìÅ Checking scripts directory...\")\n",
    "if os.path.exists(SCRIPTS_DIR):\n",
    "    scripts = [f for f in os.listdir(SCRIPTS_DIR) if f.endswith('.py')]\n",
    "    print(\"Available scripts:\")\n",
    "    for script in scripts:\n",
    "        print(f\"  - {script}\")\n",
    "else:\n",
    "    print(f\"‚ùå Directory {SCRIPTS_DIR} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddc88a-1fd7-4421-8634-795fdbc61768",
   "metadata": {},
   "source": [
    "## 2. Testing that model scripts are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f553a2-d687-41a6-a23c-f7ca5e76e71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loan Default Prediction - Training Script Tests\n",
      "üìÅ Scripts location: /app/utils\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Listing available scripts...\n",
      "üìÅ Available files in /app/utils:\n",
      "ü§ñ Training scripts:\n",
      "   - model_training_LR.py\n",
      "   - model_training_RF.py\n",
      "   - model_training_XG.py\n",
      "\n",
      "2Ô∏è‚É£ Checking for import issues...\n",
      "üîß Checking for any remaining preprocessor imports...\n",
      "‚úÖ No problematic imports found\n",
      "\n",
      "3Ô∏è‚É£ Verifying embedded preprocessing...\n",
      "üîç Verifying embedded preprocessing functions...\n",
      "‚úÖ Logistic Regression has embedded preprocess_features_for_lr()\n",
      "   ‚úÖ Has from sklearn.preprocessing import\n",
      "   ‚úÖ Has from sklearn.compose import ColumnTransformer\n",
      "‚úÖ XGBoost has embedded preprocess_features_for_tree()\n",
      "   ‚úÖ Has from sklearn.preprocessing import\n",
      "   ‚úÖ Has from sklearn.compose import ColumnTransformer\n",
      "‚úÖ Random Forest has embedded preprocess_features_for_tree()\n",
      "   ‚úÖ Has from sklearn.preprocessing import\n",
      "   ‚úÖ Has from sklearn.compose import ColumnTransformer\n",
      "\n",
      "4Ô∏è‚É£ Checking script syntax...\n",
      "üß™ Checking script syntax...\n",
      "\n",
      "üìÅ Testing Logistic Regression syntax...\n",
      "‚úÖ Logistic Regression syntax is valid\n",
      "\n",
      "üìÅ Testing XGBoost syntax...\n",
      "‚úÖ XGBoost syntax is valid\n",
      "\n",
      "üìÅ Testing Random Forest syntax...\n",
      "‚úÖ Random Forest syntax is valid\n",
      "\n",
      "5Ô∏è‚É£ Running tests...\n",
      "üî¨ Starting Training Script Tests\n",
      "üìÅ Scripts directory: /app/utils\n",
      "============================================================\n",
      "\n",
      "üß™ Testing Logistic Regression...\n",
      "üìÅ Script: /app/utils/model_training_LR.py\n",
      "‚ö° Command: /usr/local/bin/python3.12 /app/utils/model_training_LR.py --train_date 2025-09-10 --features_path /app/datamart/gold/feature_store/ --labels_path /app/datamart/gold/label_store/ --sample_frac 0.05 --n_iter 2 --cv_folds 2 --train_months 2 --val_months 1 --test_months 1 --oot_months 1 --mlflow_tracking_uri http://localhost:5000 --mlflow_experiment loan-default-testing\n",
      "üîÑ Executing script...\n",
      "‚ùå Logistic Regression - FAILED (return code: 1)\n",
      "üí• Error output:\n",
      "   2025-11-09 06:22:27,741 INFO Configuration: Train=2025-04-10‚Üí2025-06-09\n",
      "   WARNING: Using incubator modules: jdk.incubator.vector\n",
      "   Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "   Setting default log level to \"WARN\".\n",
      "   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "   25/11/09 06:22:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "   \n",
      "   [Stage 0:>                                                          (0 + 1) / 1]\n",
      "   \n",
      "                                                                                   \n",
      "   25/11/09 06:22:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "   2025-11-09 06:22:41,310 INFO Split 'train' class distribution: {}\n",
      "   2025-11-09 06:22:41,310 WARNING Split 'train' has only one class: {}\n",
      "   2025-11-09 06:22:41,869 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:22:42,388 INFO Split 'val' class distribution: {}\n",
      "   2025-11-09 06:22:42,389 WARNING Split 'val' has only one class: {}\n",
      "   \n",
      "   [Stage 6:>                                                          (0 + 1) / 1]\n",
      "   \n",
      "                                                                                   \n",
      "   2025-11-09 06:22:43,316 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:22:43,621 INFO Split 'test' class distribution: {}\n",
      "   2025-11-09 06:22:43,621 WARNING Split 'test' has only one class: {}\n",
      "   2025-11-09 06:22:43,934 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:22:44,266 INFO Split 'oot' class distribution: {}\n",
      "   2025-11-09 06:22:44,266 WARNING Split 'oot' has only one class: {}\n",
      "   2025-11-09 06:22:44,858 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:22:45,328 INFO Stopped Spark\n",
      "   Traceback (most recent call last):\n",
      "     File \"/app/utils/model_training_LR.py\", line 397, in <module>\n",
      "       main(args)\n",
      "     File \"/app/utils/model_training_LR.py\", line 287, in main\n",
      "       X_train_rf, [X_val_rf, X_test_rf, X_oot_rf], encoder, feature_names = preprocess_features_for_lr(\n",
      "                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "     File \"/app/utils/model_training_LR.py\", line 88, in preprocess_features_for_lr\n",
      "       ('cat', OneHotEncoder(drop='first', sparse=False), categorical_cols)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "   TypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\n",
      "   2025-11-09 06:22:45,383 INFO Closing down clientserver connection\n",
      "\n",
      "üß™ Testing XGBoost...\n",
      "üìÅ Script: /app/utils/model_training_XG.py\n",
      "‚ö° Command: /usr/local/bin/python3.12 /app/utils/model_training_XG.py --train_date 2025-09-10 --features_path /app/datamart/gold/feature_store/ --labels_path /app/datamart/gold/label_store/ --sample_frac 0.05 --n_iter 2 --cv_folds 2 --train_months 2 --val_months 1 --test_months 1 --oot_months 1 --mlflow_tracking_uri http://localhost:5000 --mlflow_experiment loan-default-testing\n",
      "üîÑ Executing script...\n",
      "‚ùå XGBoost - FAILED (return code: 1)\n",
      "üí• Error output:\n",
      "   2025-11-09 06:22:48,665 INFO Configuration: Train=2025-04-10‚Üí2025-06-09\n",
      "   WARNING: Using incubator modules: jdk.incubator.vector\n",
      "   Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "   Setting default log level to \"WARN\".\n",
      "   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "   25/11/09 06:22:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "   \n",
      "   [Stage 2:>                                                          (0 + 1) / 1]\n",
      "   \n",
      "                                                                                   \n",
      "   25/11/09 06:22:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "   2025-11-09 06:22:59,202 INFO Split 'train' class distribution: {}\n",
      "   2025-11-09 06:22:59,202 WARNING Split 'train' has only one class: {}\n",
      "   2025-11-09 06:22:59,838 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:00,336 INFO Split 'val' class distribution: {}\n",
      "   2025-11-09 06:23:00,336 WARNING Split 'val' has only one class: {}\n",
      "   2025-11-09 06:23:00,803 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:01,226 INFO Split 'test' class distribution: {}\n",
      "   2025-11-09 06:23:01,226 WARNING Split 'test' has only one class: {}\n",
      "   2025-11-09 06:23:01,686 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:02,007 INFO Split 'oot' class distribution: {}\n",
      "   2025-11-09 06:23:02,007 WARNING Split 'oot' has only one class: {}\n",
      "   2025-11-09 06:23:02,395 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:02,658 INFO Stopped Spark\n",
      "   Traceback (most recent call last):\n",
      "     File \"/app/utils/model_training_XG.py\", line 369, in <module>\n",
      "       main(args)\n",
      "     File \"/app/utils/model_training_XG.py\", line 252, in main\n",
      "       X_train_xgb, [X_val_xgb, X_test_xgb, X_oot_xgb], encoder, feature_names = preprocess_features_for_tree(\n",
      "                                                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "     File \"/app/utils/model_training_XG.py\", line 50, in preprocess_features_for_tree\n",
      "       ('cat', OneHotEncoder(drop='first', sparse=False), categorical_cols)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "   TypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\n",
      "   2025-11-09 06:23:02,719 INFO Closing down clientserver connection\n",
      "\n",
      "üß™ Testing Random Forest...\n",
      "üìÅ Script: /app/utils/model_training_RF.py\n",
      "‚ö° Command: /usr/local/bin/python3.12 /app/utils/model_training_RF.py --train_date 2025-09-10 --features_path /app/datamart/gold/feature_store/ --labels_path /app/datamart/gold/label_store/ --sample_frac 0.05 --n_iter 2 --cv_folds 2 --train_months 2 --val_months 1 --test_months 1 --oot_months 1 --mlflow_tracking_uri http://localhost:5000 --mlflow_experiment loan-default-testing\n",
      "üîÑ Executing script...\n",
      "‚ùå Random Forest - FAILED (return code: 1)\n",
      "üí• Error output:\n",
      "   2025-11-09 06:23:06,514 INFO Configuration: Train=2025-04-10‚Üí2025-06-09\n",
      "   WARNING: Using incubator modules: jdk.incubator.vector\n",
      "   Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "   Setting default log level to \"WARN\".\n",
      "   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "   25/11/09 06:23:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "   25/11/09 06:23:16 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "   2025-11-09 06:23:16,762 INFO Split 'train' class distribution: {}\n",
      "   2025-11-09 06:23:16,762 WARNING Split 'train' has only one class: {}\n",
      "   2025-11-09 06:23:17,229 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:17,984 INFO Split 'val' class distribution: {}\n",
      "   2025-11-09 06:23:17,984 WARNING Split 'val' has only one class: {}\n",
      "   2025-11-09 06:23:18,481 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:18,743 INFO Split 'test' class distribution: {}\n",
      "   2025-11-09 06:23:18,743 WARNING Split 'test' has only one class: {}\n",
      "   \n",
      "   [Stage 8:>                                                          (0 + 1) / 1]\n",
      "   \n",
      "                                                                                   \n",
      "   2025-11-09 06:23:19,552 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   \n",
      "   [Stage 9:>                                                          (0 + 1) / 1]\n",
      "   \n",
      "                                                                                   \n",
      "   2025-11-09 06:23:20,483 INFO Split 'oot' class distribution: {}\n",
      "   2025-11-09 06:23:20,483 WARNING Split 'oot' has only one class: {}\n",
      "   2025-11-09 06:23:20,824 INFO Converted Spark DF to pandas: (0, 51)\n",
      "   2025-11-09 06:23:21,237 INFO Stopped Spark\n",
      "   Traceback (most recent call last):\n",
      "     File \"/app/utils/model_training_RF.py\", line 359, in <module>\n",
      "       main(args)\n",
      "     File \"/app/utils/model_training_RF.py\", line 249, in main\n",
      "       X_train_rf, [X_val_rf, X_test_rf, X_oot_rf], encoder, feature_names = preprocess_features_for_tree(\n",
      "                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "     File \"/app/utils/model_training_RF.py\", line 50, in preprocess_features_for_tree\n",
      "       ('cat', OneHotEncoder(drop='first', sparse=False), categorical_cols)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "   TypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\n",
      "   2025-11-09 06:23:21,315 INFO Closing down clientserver connection\n",
      "\n",
      "============================================================\n",
      "üìä TEST SUMMARY\n",
      "============================================================\n",
      "‚úÖ Success: 0/3\n",
      "‚ùå Logistic Regression: FAILED\n",
      "‚ùå XGBoost: FAILED\n",
      "‚ùå Random Forest: FAILED\n",
      "\n",
      "üéâ Testing complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Test script for loan default prediction training pipelines\n",
    "Run this in Jupyter Lab to test all three model training scripts in /app/utils/\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set the scripts directory\n",
    "SCRIPTS_DIR = \"/app/utils\"\n",
    "\n",
    "def test_training_scripts():\n",
    "    \"\"\"Test all three training scripts with sample parameters\"\"\"\n",
    "    \n",
    "    # Get current date and calculate a training date (2 months ago for realistic data)\n",
    "    test_date = (datetime.now() - timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Base command parameters\n",
    "    base_params = {\n",
    "        'train_date': test_date,\n",
    "        'features_path': '/app/datamart/gold/feature_store/',\n",
    "        'labels_path': '/app/datamart/gold/label_store/',\n",
    "        'sample_frac': 0.05,  # Very small sample for quick testing\n",
    "        'n_iter': 2,          # Minimal iterations for testing\n",
    "        'cv_folds': 2,        # Minimal folds for testing\n",
    "        'train_months': 2,\n",
    "        'val_months': 1,\n",
    "        'test_months': 1,\n",
    "        'oot_months': 1,\n",
    "        'mlflow_tracking_uri': 'http://localhost:5000',\n",
    "        'mlflow_experiment': 'loan-default-testing'\n",
    "    }\n",
    "    \n",
    "    scripts_to_test = [\n",
    "        {\n",
    "            'name': 'Logistic Regression',\n",
    "            'script': 'model_training_LR.py',\n",
    "            'params': base_params.copy()\n",
    "        },\n",
    "        {\n",
    "            'name': 'XGBoost', \n",
    "            'script': 'model_training_XG.py',\n",
    "            'params': base_params.copy()\n",
    "        },\n",
    "        {\n",
    "            'name': 'Random Forest',\n",
    "            'script': 'model_training_RF.py',\n",
    "            'params': base_params.copy()\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"üî¨ Starting Training Script Tests\")\n",
    "    print(f\"üìÅ Scripts directory: {SCRIPTS_DIR}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for script_info in scripts_to_test:\n",
    "        script_name = script_info['name']\n",
    "        script_file = script_info['script']\n",
    "        script_path = os.path.join(SCRIPTS_DIR, script_file)\n",
    "        params = script_info['params']\n",
    "        \n",
    "        print(f\"\\nüß™ Testing {script_name}...\")\n",
    "        print(f\"üìÅ Script: {script_path}\")\n",
    "        \n",
    "        # Check if script exists\n",
    "        if not os.path.exists(script_path):\n",
    "            print(f\"‚ùå Script not found: {script_path}\")\n",
    "            results[script_name] = {\n",
    "                'status': 'SCRIPT_NOT_FOUND',\n",
    "                'returncode': None,\n",
    "                'stdout': '',\n",
    "                'stderr': f'Script file {script_path} not found'\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Build command\n",
    "        cmd = [sys.executable, script_path]\n",
    "        \n",
    "        # Add parameters\n",
    "        for key, value in params.items():\n",
    "            cmd.append(f\"--{key}\")\n",
    "            cmd.append(str(value))\n",
    "        \n",
    "        print(f\"‚ö° Command: {' '.join(cmd)}\")\n",
    "        \n",
    "        try:\n",
    "            # Run the script\n",
    "            print(\"üîÑ Executing script...\")\n",
    "            result = subprocess.run(\n",
    "                cmd, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                timeout=300,  # 5 minute timeout\n",
    "                cwd=SCRIPTS_DIR  # Run from scripts directory to help with imports\n",
    "            )\n",
    "            \n",
    "            # Check results\n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ {script_name} - SUCCESS\")\n",
    "                print(\"üìù Output snippet:\")\n",
    "                output_lines = result.stdout.strip().split('\\n')\n",
    "                for line in output_lines[-10:]:\n",
    "                    print(f\"   {line}\")\n",
    "                \n",
    "                results[script_name] = {\n",
    "                    'status': 'SUCCESS',\n",
    "                    'returncode': result.returncode,\n",
    "                    'stdout': result.stdout,\n",
    "                    'stderr': result.stderr\n",
    "                }\n",
    "            else:\n",
    "                print(f\"‚ùå {script_name} - FAILED (return code: {result.returncode})\")\n",
    "                print(\"üí• Error output:\")\n",
    "                error_lines = result.stderr.strip().split('\\n')\n",
    "                for line in error_lines:\n",
    "                    print(f\"   {line}\")\n",
    "                \n",
    "                results[script_name] = {\n",
    "                    'status': 'FAILED',\n",
    "                    'returncode': result.returncode,\n",
    "                    'stdout': result.stdout,\n",
    "                    'stderr': result.stderr\n",
    "                }\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"‚è∞ {script_name} - TIMEOUT (exceeded 5 minutes)\")\n",
    "            results[script_name] = {\n",
    "                'status': 'TIMEOUT',\n",
    "                'returncode': None,\n",
    "                'stdout': '',\n",
    "                'stderr': 'Execution timed out'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"üö® {script_name} - UNEXPECTED ERROR: {e}\")\n",
    "            results[script_name] = {\n",
    "                'status': 'ERROR',\n",
    "                'returncode': None,\n",
    "                'stdout': '',\n",
    "                'stderr': str(e)\n",
    "            }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä TEST SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    success_count = sum(1 for result in results.values() if result['status'] == 'SUCCESS')\n",
    "    total_count = len(results)\n",
    "    \n",
    "    print(f\"‚úÖ Success: {success_count}/{total_count}\")\n",
    "    \n",
    "    for script_name, result in results.items():\n",
    "        status_icon = \"‚úÖ\" if result['status'] == 'SUCCESS' else \"‚ùå\"\n",
    "        print(f\"{status_icon} {script_name}: {result['status']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_script_syntax():\n",
    "    \"\"\"Check syntax of training scripts\"\"\"\n",
    "    print(\"üß™ Checking script syntax...\")\n",
    "    \n",
    "    scripts_to_test = [\n",
    "        ('model_training_LR.py', 'Logistic Regression'),\n",
    "        ('model_training_XG.py', 'XGBoost'),\n",
    "        ('model_training_RF.py', 'Random Forest')\n",
    "    ]\n",
    "    \n",
    "    all_valid = True\n",
    "    \n",
    "    for script_file, script_name in scripts_to_test:\n",
    "        script_path = os.path.join(SCRIPTS_DIR, script_file)\n",
    "        if os.path.exists(script_path):\n",
    "            print(f\"\\nüìÅ Testing {script_name} syntax...\")\n",
    "            try:\n",
    "                with open(script_path, 'r') as f:\n",
    "                    content = f.read()\n",
    "                # Try to compile the script to check syntax\n",
    "                compile(content, script_file, 'exec')\n",
    "                print(f\"‚úÖ {script_name} syntax is valid\")\n",
    "                \n",
    "                # Check for common issues\n",
    "                if 'model_preprocessor' in content:\n",
    "                    print(f\"‚ö†Ô∏è  {script_name} still contains reference to model_preprocessor\")\n",
    "                    all_valid = False\n",
    "                if 'from model_preprocessor' in content:\n",
    "                    print(f\"‚ö†Ô∏è  {script_name} still contains reference to model_preprocessor\")\n",
    "                    all_valid = False\n",
    "                    \n",
    "            except SyntaxError as e:\n",
    "                print(f\"‚ùå {script_name} syntax error: {e}\")\n",
    "                print(f\"   Line {e.lineno}: {e.text}\")\n",
    "                all_valid = False\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  {script_name} other error: {e}\")\n",
    "                all_valid = False\n",
    "        else:\n",
    "            print(f\"‚ùå {script_name} not found: {script_path}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "def verify_embedded_preprocessing():\n",
    "    \"\"\"Verify that preprocessing functions are embedded in scripts\"\"\"\n",
    "    print(\"üîç Verifying embedded preprocessing functions...\")\n",
    "    \n",
    "    scripts_to_check = [\n",
    "        ('model_training_LR.py', 'Logistic Regression', 'preprocess_features_for_lr'),\n",
    "        ('model_training_XG.py', 'XGBoost', 'preprocess_features_for_tree'),\n",
    "        ('model_training_RF.py', 'Random Forest', 'preprocess_features_for_tree')\n",
    "    ]\n",
    "    \n",
    "    all_have_functions = True\n",
    "    \n",
    "    for script_file, script_name, expected_function in scripts_to_check:\n",
    "        script_path = os.path.join(SCRIPTS_DIR, script_file)\n",
    "        if os.path.exists(script_path):\n",
    "            with open(script_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            if f'def {expected_function}' in content:\n",
    "                print(f\"‚úÖ {script_name} has embedded {expected_function}()\")\n",
    "            else:\n",
    "                print(f\"‚ùå {script_name} missing embedded {expected_function}()\")\n",
    "                all_have_functions = False\n",
    "                \n",
    "            # Check for required imports\n",
    "            required_imports = [\n",
    "                'from sklearn.preprocessing import',\n",
    "                'from sklearn.compose import ColumnTransformer'\n",
    "            ]\n",
    "            \n",
    "            for required_import in required_imports:\n",
    "                if required_import in content:\n",
    "                    print(f\"   ‚úÖ Has {required_import}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Missing {required_import}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {script_name} not found: {script_path}\")\n",
    "            all_have_functions = False\n",
    "    \n",
    "    return all_have_functions\n",
    "\n",
    "def list_available_scripts():\n",
    "    \"\"\"List all available training scripts\"\"\"\n",
    "    print(f\"üìÅ Available files in {SCRIPTS_DIR}:\")\n",
    "    \n",
    "    if not os.path.exists(SCRIPTS_DIR):\n",
    "        print(f\"‚ùå Directory {SCRIPTS_DIR} does not exist\")\n",
    "        return []\n",
    "    \n",
    "    files = os.listdir(SCRIPTS_DIR)\n",
    "    training_scripts = []\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.py') and 'training' in file.lower():\n",
    "            training_scripts.append(file)\n",
    "    \n",
    "    print(\"ü§ñ Training scripts:\")\n",
    "    for script in sorted(training_scripts):\n",
    "        print(f\"   - {script}\")\n",
    "    \n",
    "    return training_scripts\n",
    "\n",
    "def quick_fix_imports():\n",
    "    \"\"\"Quick fix for any remaining preprocessor imports\"\"\"\n",
    "    print(\"üîß Checking for any remaining preprocessor imports...\")\n",
    "    \n",
    "    scripts_to_fix = [\n",
    "        'model_training_LR.py',\n",
    "        'model_training_XG.py',\n",
    "        'model_training_RF.py'\n",
    "    ]\n",
    "    \n",
    "    fixed_count = 0\n",
    "    for script_file in scripts_to_fix:\n",
    "        script_path = os.path.join(SCRIPTS_DIR, script_file)\n",
    "        if os.path.exists(script_path):\n",
    "            with open(script_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Check for problematic imports\n",
    "            problematic_patterns = [\n",
    "                'from model_preprocessor import',\n",
    "                'from model_preprocessor import'\n",
    "            ]\n",
    "            \n",
    "            has_problem = False\n",
    "            for pattern in problematic_patterns:\n",
    "                if pattern in content:\n",
    "                    print(f\"‚ö†Ô∏è  Found problematic import in {script_file}: {pattern}\")\n",
    "                    has_problem = True\n",
    "            \n",
    "            if has_problem:\n",
    "                print(f\"üí° {script_file} still has external preprocessor imports\")\n",
    "                fixed_count += 1\n",
    "    \n",
    "    if fixed_count > 0:\n",
    "        print(f\"\\nüîß {fixed_count} scripts still need manual fixing\")\n",
    "        print(\"üí° Make sure each script has embedded preprocessing functions\")\n",
    "    else:\n",
    "        print(\"‚úÖ No problematic imports found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Loan Default Prediction - Training Script Tests\")\n",
    "    print(f\"üìÅ Scripts location: {SCRIPTS_DIR}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, list available scripts\n",
    "    print(\"\\n1Ô∏è‚É£ Listing available scripts...\")\n",
    "    training_scripts = list_available_scripts()\n",
    "    \n",
    "    if not training_scripts:\n",
    "        print(\"‚ùå No training scripts found. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Check for any remaining import issues\n",
    "    print(\"\\n2Ô∏è‚É£ Checking for import issues...\")\n",
    "    quick_fix_imports()\n",
    "    \n",
    "    # Verify embedded preprocessing\n",
    "    print(\"\\n3Ô∏è‚É£ Verifying embedded preprocessing...\")\n",
    "    preprocessing_ok = verify_embedded_preprocessing()\n",
    "    \n",
    "    # Check syntax\n",
    "    print(\"\\n4Ô∏è‚É£ Checking script syntax...\")\n",
    "    syntax_ok = check_script_syntax()\n",
    "    \n",
    "    # Run tests only if everything looks good\n",
    "    if preprocessing_ok and syntax_ok:\n",
    "        print(\"\\n5Ô∏è‚É£ Running tests...\")\n",
    "        results = test_training_scripts()\n",
    "    else:\n",
    "        print(\"\\n‚ùå Skipping tests due to script issues\")\n",
    "        print(\"üí° Please fix the issues above before running tests\")\n",
    "        results = {}\n",
    "    \n",
    "    print(\"\\nüéâ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f3b24-6c76-473b-8917-0b1099fda641",
   "metadata": {},
   "source": [
    "## 3. Checking Gold Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ae8940-5d89-462e-9129-950a7c1f9767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 06:24:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking data availability:\n",
      "Features path: /app/datamart/gold/feature_store/\n",
      "Labels path: /app/datamart/gold/label_store/\n",
      "‚úÖ Features directory exists\n",
      "   Files: ['.part-00000-28a5c4b2-198b-4954-8889-e2021b9a951b-c000.snappy.parquet.crc', '._SUCCESS.crc', 'part-00000-28a5c4b2-198b-4954-8889-e2021b9a951b-c000.snappy.parquet', '_SUCCESS']...\n",
      "‚úÖ Labels directory exists\n",
      "   Files: ['.part-00000-6deaf5e4-8c74-40b8-ae60-6f29e7bbf25d-c000.snappy.parquet.crc', '._SUCCESS.crc', 'part-00000-6deaf5e4-8c74-40b8-ae60-6f29e7bbf25d-c000.snappy.parquet', '_SUCCESS']...\n",
      "\n",
      "üìä Trying to read features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features schema: ['Customer_ID', 'snapshot_date', 'feature_snapshot_date', 'Age', 'Occupation', 'Delay_from_due_date', 'Outstanding_Debt', 'Amount_invested_monthly', 'Interest_Rate', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Loan_Type_Home_Loan', 'Loan_Type_Personal_Loan', 'Loan_Type_Student_Loan', 'Loan_Type_Auto_Loan', 'Loan_Type_Business_Loan', 'Loan_Type_Credit-Builder_Loan', 'Loan_Type_Home_Equity_Loan', 'Loan_Type_Debt_Consolidation_Loan', 'Loan_Type_Mortgage_Loan', 'Loan_Type_Not_Specified', 'Loan_Type_Payday_Loan', 'Loan_amt_sum', 'Loan_amt_mean', 'Loan_amt_std', 'Loan_tenure_mean', 'Loan_tenure_max', 'Loan_overdue_amt_sum', 'Loan_overdue_amt_mean', 'Loan_overdue_amt_max', 'Loan_balance_sum', 'Loan_balance_mean', 'dpd_mean', 'dpd_max', 'loan_count', 'clickstream_total_events', 'clickstream_fe_5_mean', 'clickstream_fe_5_sum', 'clickstream_fe_5_std', 'clickstream_fe_9_mean', 'clickstream_fe_9_min', 'clickstream_fe_4_mean', 'clickstream_fe_4_min', 'clickstream_fe_10_mean', 'clickstream_fe_10_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features count: 5531\n",
      "Sample features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 06:25:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+----------+-------------------+----------------+-----------------------+-------------+-----------------+---------------+-------------------+-----------------------+----------------------+-------------------+-----------------------+-----------------------------+--------------------------+---------------------------------+-----------------------+-----------------------+---------------------+------------+-------------+------------+----------------+---------------+--------------------+---------------------+--------------------+----------------+-----------------+--------+-------+----------+------------------------+---------------------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "|Customer_ID|snapshot_date|feature_snapshot_date|Age|Occupation|Delay_from_due_date|Outstanding_Debt|Amount_invested_monthly|Interest_Rate|Num_Bank_Accounts|Num_Credit_Card|Loan_Type_Home_Loan|Loan_Type_Personal_Loan|Loan_Type_Student_Loan|Loan_Type_Auto_Loan|Loan_Type_Business_Loan|Loan_Type_Credit-Builder_Loan|Loan_Type_Home_Equity_Loan|Loan_Type_Debt_Consolidation_Loan|Loan_Type_Mortgage_Loan|Loan_Type_Not_Specified|Loan_Type_Payday_Loan|Loan_amt_sum|Loan_amt_mean|Loan_amt_std|Loan_tenure_mean|Loan_tenure_max|Loan_overdue_amt_sum|Loan_overdue_amt_mean|Loan_overdue_amt_max|Loan_balance_sum|Loan_balance_mean|dpd_mean|dpd_max|loan_count|clickstream_total_events|clickstream_fe_5_mean|clickstream_fe_5_sum|clickstream_fe_5_std|clickstream_fe_9_mean|clickstream_fe_9_min|clickstream_fe_4_mean|clickstream_fe_4_min|clickstream_fe_10_mean|clickstream_fe_10_min|\n",
      "+-----------+-------------+---------------------+---+----------+-------------------+----------------+-----------------------+-------------+-----------------+---------------+-------------------+-----------------------+----------------------+-------------------+-----------------------+-----------------------------+--------------------------+---------------------------------+-----------------------+-----------------------+---------------------+------------+-------------+------------+----------------+---------------+--------------------+---------------------+--------------------+----------------+-----------------+--------+-------+----------+------------------------+---------------------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "| CUS_0x100b|   2024-12-01|           2024-12-01|  0|   Unknown|                  0|             0.0|                    0.0|          0.0|                0|              0|                  0|                      0|                     0|                  0|                      0|                            0|                         0|                                0|                      0|                      0|                    0|     10000.0|      10000.0|         0.0|            10.0|             10|                 0.0|                  0.0|                 0.0|          1000.0|           1000.0|     0.0|      0|         1|                       1|                 98.0|                  98|                 0.0|                 71.0|                  71|                -18.0|                 -18|                   1.0|                    1|\n",
      "| CUS_0x102e|   2024-12-01|           2024-12-01|  0|   Unknown|                  0|             0.0|                    0.0|          0.0|                0|              0|                  0|                      0|                     0|                  0|                      0|                            0|                         0|                                0|                      0|                      0|                    0|     10000.0|      10000.0|         0.0|            10.0|             10|              6000.0|               6000.0|              6000.0|          8000.0|           8000.0|   180.0|    180|         1|                       1|                209.0|                 209|                 0.0|                161.0|                 161|                 23.0|                  23|                 119.0|                  119|\n",
      "| CUS_0x1038|   2024-12-01|           2024-12-01|  0|   Unknown|                  0|             0.0|                    0.0|          0.0|                0|              0|                  0|                      0|                     0|                  0|                      0|                            0|                         0|                                0|                      0|                      0|                    0|     10000.0|      10000.0|         0.0|            10.0|             10|                 0.0|                  0.0|                 0.0|          8000.0|           8000.0|     0.0|      0|         1|                    NULL|                  0.0|                NULL|                 0.0|                  0.0|                   0|                  0.0|                   0|                   0.0|                    0|\n",
      "| CUS_0x103e|   2024-12-01|           2024-12-01| 40| Scientist|                  6|          706.96|               913.4813|          9.0|                4|              6|                  0|                      0|                     1|                  0|                      0|                            0|                         0|                                0|                      0|                      0|                    0|     10000.0|      10000.0|         0.0|            10.0|             10|                 0.0|                  0.0|                 0.0|         10000.0|          10000.0|     0.0|      0|         1|                    NULL|                  0.0|                NULL|                 0.0|                  0.0|                   0|                  0.0|                   0|                   0.0|                    0|\n",
      "| CUS_0x1048|   2024-12-01|           2024-12-01|  0|   Unknown|                  0|             0.0|                    0.0|          0.0|                0|              0|                  0|                      0|                     0|                  0|                      0|                            0|                         0|                                0|                      0|                      0|                    0|     10000.0|      10000.0|         0.0|            10.0|             10|              9000.0|               9000.0|              9000.0|          9000.0|           9000.0|   270.0|    270|         1|                       1|                280.0|                 280|                 0.0|                 84.0|                  84|                102.0|                 102|                 118.0|                  118|\n",
      "+-----------+-------------+---------------------+---+----------+-------------------+----------------+-----------------------+-------------+-----------------+---------------+-------------------+-----------------------+----------------------+-------------------+-----------------------+-----------------------------+--------------------------+---------------------------------+-----------------------+-----------------------+---------------------+------------+-------------+------------+----------------+---------------+--------------------+---------------------+--------------------+----------------+-----------------+--------+-------+----------+------------------------+---------------------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "\n",
      "\n",
      "üìä Trying to read labels...\n",
      "Labels schema: ['loan_id', 'Customer_ID', 'label', 'label_definition', 'label_snapshot_date', 'snapshot_date', 'mob', 'dpd']\n",
      "Labels count: 2531\n",
      "Sample labels:\n",
      "+--------------------+-----------+-----+----------------+-------------------+-------------+---+---+\n",
      "|             loan_id|Customer_ID|label|label_definition|label_snapshot_date|snapshot_date|mob|dpd|\n",
      "+--------------------+-----------+-----+----------------+-------------------+-------------+---+---+\n",
      "|CUS_0x100b_2024_0...| CUS_0x100b|    0|  DPD_30+_MOB_6+|         2024-12-01|   2024-12-01|  9|  0|\n",
      "|CUS_0x102e_2024_0...| CUS_0x102e|    1|  DPD_30+_MOB_6+|         2024-12-01|   2024-12-01|  8|180|\n",
      "|CUS_0x1048_2024_0...| CUS_0x1048|    1|  DPD_30+_MOB_6+|         2024-12-01|   2024-12-01| 10|270|\n",
      "|CUS_0x1075_2024_0...| CUS_0x1075|    1|  DPD_30+_MOB_6+|         2024-12-01|   2024-12-01|  7| 90|\n",
      "|CUS_0x1096_2024_0...| CUS_0x1096|    1|  DPD_30+_MOB_6+|         2024-12-01|   2024-12-01|  9|180|\n",
      "+--------------------+-----------+-----+----------------+-------------------+-------------+---+---+\n",
      "\n",
      "\n",
      "üìÖ Date ranges in features:\n",
      "+-------------+\n",
      "|snapshot_date|\n",
      "+-------------+\n",
      "|   2024-12-01|\n",
      "+-------------+\n",
      "\n",
      "üìÖ Date ranges in labels:\n",
      "+-------------+\n",
      "|snapshot_date|\n",
      "+-------------+\n",
      "|   2024-12-01|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data_debug\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check what files exist\n",
    "features_path = '/app/datamart/gold/feature_store/'\n",
    "labels_path = '/app/datamart/gold/label_store/'\n",
    "\n",
    "print(\"üîç Checking data availability:\")\n",
    "print(f\"Features path: {features_path}\")\n",
    "print(f\"Labels path: {labels_path}\")\n",
    "\n",
    "import os\n",
    "if os.path.exists(features_path):\n",
    "    print(f\"‚úÖ Features directory exists\")\n",
    "    files = os.listdir(features_path)\n",
    "    print(f\"   Files: {files[:5]}...\")  # Show first 5 files\n",
    "else:\n",
    "    print(f\"‚ùå Features directory does not exist\")\n",
    "\n",
    "if os.path.exists(labels_path):\n",
    "    print(f\"‚úÖ Labels directory exists\")\n",
    "    files = os.listdir(labels_path)\n",
    "    print(f\"   Files: {files[:5]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå Labels directory does not exist\")\n",
    "\n",
    "# Try to read a small sample of data\n",
    "try:\n",
    "    print(\"\\nüìä Trying to read features...\")\n",
    "    features_sdf = spark.read.parquet(features_path)\n",
    "    print(f\"Features schema: {features_sdf.columns}\")\n",
    "    print(f\"Features count: {features_sdf.count()}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if features_sdf.count() > 0:\n",
    "        print(\"Sample features:\")\n",
    "        features_sdf.limit(5).show()\n",
    "    else:\n",
    "        print(\"‚ùå No features data found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading features: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nüìä Trying to read labels...\")\n",
    "    labels_sdf = spark.read.parquet(labels_path)\n",
    "    print(f\"Labels schema: {labels_sdf.columns}\")\n",
    "    print(f\"Labels count: {labels_sdf.count()}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if labels_sdf.count() > 0:\n",
    "        print(\"Sample labels:\")\n",
    "        labels_sdf.limit(5).show()\n",
    "    else:\n",
    "        print(\"‚ùå No labels data found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading labels: {e}\")\n",
    "\n",
    "# Check date ranges if data exists\n",
    "try:\n",
    "    if features_sdf.count() > 0 and 'snapshot_date' in features_sdf.columns:\n",
    "        print(\"\\nüìÖ Date ranges in features:\")\n",
    "        features_sdf.select(\"snapshot_date\").distinct().orderBy(\"snapshot_date\").show()\n",
    "        \n",
    "    if labels_sdf.count() > 0 and 'snapshot_date' in labels_sdf.columns:\n",
    "        print(\"üìÖ Date ranges in labels:\")\n",
    "        labels_sdf.select(\"snapshot_date\").distinct().orderBy(\"snapshot_date\").show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking dates: {e}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d7d411-e87b-4b91-a052-995371b11aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (706167350.py, line 15)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m.appName(\"GoldDataDiagnostic\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Create a diagnostic script to check the gold layer data\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/app')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "\n",
    "def check_gold_data_quality():\n",
    "    spark = SparkSession.builder \n",
    "        .appName(\"GoldDataDiagnostic\") \n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Check feature store\n",
    "    features_path = \"/app/datamart/gold/feature_store/\"\n",
    "    labels_path = \"/app/datamart/gold/label_store/\"\n",
    "    \n",
    "    print(\"üîç Checking Gold Layer Data Quality...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if feature store exists and has data\n",
    "        print(f\"üìÅ Checking feature store: {features_path}\")\n",
    "        feature_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark._jsc.hadoopConfiguration()\n",
    "        ).listStatus(spark._jvm.org.apache.hadoop.fs.Path(features_path))\n",
    "        \n",
    "        if feature_files:\n",
    "            print(\"‚úÖ Feature store exists\")\n",
    "            # Try to read a sample of features\n",
    "            features_df = spark.read.parquet(features_path).limit(10)\n",
    "            print(f\"üìä Features sample count: {features_df.count()}\")\n",
    "            print(\"üìã Features schema:\")\n",
    "            features_df.printSchema()\n",
    "            print(\"üî¢ Features sample data:\")\n",
    "            features_df.show(5, truncate=False)\n",
    "            \n",
    "            # Check for nulls and data issues\n",
    "            print(\"‚ùì Checking for data issues in features:\")\n",
    "            for col_name in features_df.columns:\n",
    "                null_count = features_df.filter(F.col(col_name).isNull()).count()\n",
    "                print(f\"   {col_name}: {null_count} nulls\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Feature store is empty or doesn't exist\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading feature store: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if label store exists and has data\n",
    "        print(f\"\\\\nüìÅ Checking label store: {labels_path}\")\n",
    "        label_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark._jsc.hadoopConfiguration()\n",
    "        ).listStatus(spark._jvm.org.apache.hadoop.fs.Path(labels_path))\n",
    "        \n",
    "        if label_files:\n",
    "            print(\"‚úÖ Label store exists\")\n",
    "            # Try to read a sample of labels\n",
    "            labels_df = spark.read.parquet(labels_path).limit(10)\n",
    "            print(f\"üìä Labels sample count: {labels_df.count()}\")\n",
    "            print(\"üìã Labels schema:\")\n",
    "            labels_df.printSchema()\n",
    "            print(\"üî¢ Labels sample data:\")\n",
    "            labels_df.show(5, truncate=False)\n",
    "            \n",
    "            # Check label distribution\n",
    "            if \"default_flag\" in labels_df.columns:\n",
    "                label_dist = labels_df.groupBy(\"default_flag\").count().collect()\n",
    "                print(\"üìà Label distribution:\")\n",
    "                for row in label_dist:\n",
    "                    print(f\"   default_flag {row['default_flag']}: {row['count']} records\")\n",
    "            else:\n",
    "                print(\"‚ùå 'default_flag' column not found in labels\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Label store is empty or doesn't exist\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading label store: {e}\")\n",
    "    \n",
    "    # Check specific date range mentioned in the training\n",
    "    train_date = \"2025-09-10\"\n",
    "    print(f\"\\\\nüìÖ Checking data for training date: {train_date}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if we have features for the required time periods\n",
    "        features_full = spark.read.parquet(features_path)\n",
    "        print(f\"üìä Total features records: {features_full.count()}\")\n",
    "        \n",
    "        # Check date range\n",
    "        if \"snapshot_date\" in features_full.columns:\n",
    "            date_range = features_full.agg(\n",
    "                F.min(\"snapshot_date\").alias(\"min_date\"),\n",
    "                F.max(\"snapshot_date\").alias(\"max_date\")\n",
    "            ).collect()[0]\n",
    "            print(f\"üìÖ Features date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing features: {e}\")\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gold_data_quality()\n",
    "\n",
    "\n",
    "# Save and run the diagnostic script\n",
    "with open('/app/utils/check_gold_data.py', 'w') as f:\n",
    "    f.write(diagnostic_script)\n",
    "\n",
    "print(\"üîç Running gold data diagnostic...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe32c5e-cb83-4b4d-9978-dae66c1a9b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
