# Section 1: Directory Tree ğŸ“‚
```
assign_2/
â”‚
â”œâ”€â”€ data/                       # Datasets
â”‚   â”œâ”€â”€ feature_clickstream.csv
â”‚   â”œâ”€â”€ features_attributes.csv
â”‚   â”œâ”€â”€ features_financials.csv
â”‚   â””â”€â”€ lms_loan_daily.csv
â”‚
â”œâ”€â”€ datamart/                   # Medallion data warehouse
â”‚   â”œâ”€â”€ bronze/ (csv, by year and month)
â”‚   â”œâ”€â”€ silver/ (Parquet)
â”‚   â””â”€â”€ gold/   (Parquet, feature_store/, label_store/)
â”‚
â”œâ”€â”€ airflow/                    # Airflow setup + config
â”œâ”€â”€ dags/                       # DAGs: data pipeline, training, monitoring
â”‚
â”œâ”€â”€ notebooks/                  # Dev notebooks
â”‚
â”œâ”€â”€ mlflow/                     # MLflow tracking/experiments
â”‚
â”œâ”€â”€ utils/			# PySpark + ML training scripts
â”‚   â”œâ”€â”€ processing_bronze_table.py
â”‚   â”œâ”€â”€ processing_silver_table.py
â”‚   â”œâ”€â”€ processing_gold_table.py
â”‚   â”œâ”€â”€ model_training_LR.py
â”‚   â”œâ”€â”€ model_training_XG.py
â”‚   â””â”€â”€ model_training_RF.py
â”‚
â”œâ”€â”€ docker-compose.yaml         # Airflow + MLflow orchestration
â”‚
â”œâ”€â”€ data_processing_pipeline.py     # Bronze/Silver/Gold pipeline (ETL)
â”œâ”€â”€ model_training_pipeline.py        # Training + MLflow registration
â”œâ”€â”€ inference_and_monitoring_pipeline.py      # Batch/online inference + monitoring
â”‚
â””â”€â”€ README.md

```
# Section 2: How to Run 
## 1ï¸âƒ£ Start Environment

Make sure you have Docker + Docker Compose installed.  
Build and start all services (Airflow, MLflow, JupyterLab):
```bash
docker-compose up --build
```
Once started:  
| Service                | URL                                            |
| ---------------------- | ---------------------------------------------- |
| **Airflow Web UI**     | [http://localhost:8080](http://localhost:8080) |
| **MLflow Tracking UI** | [http://localhost:5000](http://localhost:5000) |
| **JupyterLab**         | [http://localhost:8888](http://localhost:8888) |


## 2ï¸âƒ£ Run Data Pipeline

### Option A â€“ via Airflow (Recommended)
Airflow DAGs are located in /dags:  
| DAG                                 | Purpose                                   |
| ----------------------------------- | ----------------------------------------- |
| `data_pipeline_dag.py`              | ETL pipeline (Bronze â†’ Silver â†’ Gold)     |
| `scheduled_training_dag.py`         | Scheduled model training & MLflow logging |
| `daily_inference_monitoring_dag.py` | Daily inference + model monitoring        |
  

Steps:  
	1.	Open Airflow UI (http://localhost:8080)  
	2.	Trigger the DAG manually or let it run on schedule  

### Option B â€“ via Python Scripts
Run specific stages manually:

Or run each script: 


```bash
python data_processing_pipeline.py 
```
Creates Bronze â†’ Silver â†’ Gold tables (full ETL)
```bash
python model_training_pipeline.py 2024-12-01
```
Trains the model for a chosen date and logs results to MLflow.
```bash
python nference_and_monitoring_pipeline.py
``` 
Runs inference and performs drift + performance monitoring.



