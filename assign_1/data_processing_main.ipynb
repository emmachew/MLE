{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdateutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelativedelta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relativedelta\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "import utils.data_processing_bronze_table\n",
    "import utils.data_processing_silver_table\n",
    "import utils.data_processing_gold_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3faa55b5-effe-4886-bc23-9fa8852dd5aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySpark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpyspark\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(f\"PySpark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32fb3bc6-4166-4893-88e1-0d3140df5a92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Set log level to ERROR to hide warnings\u001b[39;00m\n\u001b[0;32m      8\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30206071-5f00-4c3b-be13-55c54db8e336",
   "metadata": {},
   "source": [
    "## set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0f3a464-fe45-477b-8815-cebe102228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up config\n",
    "snapshot_date_str = \"2023-01-01\"\n",
    "\n",
    "start_date_str = \"2023-01-01\"\n",
    "end_date_str = \"2024-12-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf8787cd-0a84-4332-b2ab-a9efdc2a597a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-01-01',\n",
       " '2023-02-01',\n",
       " '2023-03-01',\n",
       " '2023-04-01',\n",
       " '2023-05-01',\n",
       " '2023-06-01',\n",
       " '2023-07-01',\n",
       " '2023-08-01',\n",
       " '2023-09-01',\n",
       " '2023-10-01',\n",
       " '2023-11-01',\n",
       " '2023-12-01',\n",
       " '2024-01-01',\n",
       " '2024-02-01',\n",
       " '2024-03-01',\n",
       " '2024-04-01',\n",
       " '2024-05-01',\n",
       " '2024-06-01',\n",
       " '2024-07-01',\n",
       " '2024-08-01',\n",
       " '2024-09-01',\n",
       " '2024-10-01',\n",
       " '2024-11-01',\n",
       " '2024-12-01']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate list of dates to process\n",
    "def generate_first_of_month_dates(start_date_str, end_date_str):\n",
    "    # Convert the date strings to datetime objects\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "    \n",
    "    # List to store the first of month dates\n",
    "    first_of_month_dates = []\n",
    "\n",
    "    # Start from the first of the month of the start_date\n",
    "    current_date = datetime(start_date.year, start_date.month, 1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        # Append the date in yyyy-mm-dd format\n",
    "        first_of_month_dates.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Move to the first of the next month\n",
    "        if current_date.month == 12:\n",
    "            current_date = datetime(current_date.year + 1, 1, 1)\n",
    "        else:\n",
    "            current_date = datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    return first_of_month_dates\n",
    "\n",
    "dates_str_lst = generate_first_of_month_dates(start_date_str, end_date_str)\n",
    "dates_str_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cddd1-b87b-42dd-8158-8ecf9bd6b839",
   "metadata": {},
   "source": [
    "## Build Bronze Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c1874a1-7485-4d53-8d21-f288ad309a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bronze datalake\n",
    "bronze_lms_directory = \"datamart/bronze/lms/\"\n",
    "\n",
    "if not os.path.exists(bronze_lms_directory):\n",
    "    os.makedirs(bronze_lms_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef27a306-7fdc-4c82-aa84-5b9703fc2e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run bronze backfill\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date_str \u001b[38;5;129;01min\u001b[39;00m dates_str_lst:\n\u001b[1;32m----> 3\u001b[0m     utils\u001b[38;5;241m.\u001b[39mdata_processing_bronze_table\u001b[38;5;241m.\u001b[39mprocess_bronze_table(date_str, bronze_lms_directory, spark)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# run bronze backfill\n",
    "for date_str in dates_str_lst:\n",
    "    utils.data_processing_bronze_table.process_bronze_table(date_str, bronze_lms_directory, spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3ea424c9-8361-4a66-977a-4850c0962941",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# inspect output\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m utils\u001b[38;5;241m.\u001b[39mdata_processing_bronze_table\u001b[38;5;241m.\u001b[39mprocess_bronze_table(date_str, bronze_lms_directory, spark)\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# inspect output\n",
    "utils.data_processing_bronze_table.process_bronze_table(date_str, bronze_lms_directory, spark).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767bf08-b027-43bd-a695-43b5217fd756",
   "metadata": {},
   "source": [
    "## Build Silver Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81fd1733-5963-4536-987b-528e045dba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bronze datalake\n",
    "silver_loan_daily_directory = \"datamart/silver/loan_daily/\"\n",
    "\n",
    "if not os.path.exists(silver_loan_daily_directory):\n",
    "    os.makedirs(silver_loan_daily_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ccd38ec-e3f9-4c4e-8128-09b521f0f1b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run silver backfill\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date_str \u001b[38;5;129;01min\u001b[39;00m dates_str_lst:\n\u001b[1;32m----> 3\u001b[0m     utils\u001b[38;5;241m.\u001b[39mdata_processing_silver_table\u001b[38;5;241m.\u001b[39mprocess_silver_table(date_str, bronze_lms_directory, silver_loan_daily_directory, spark)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# run silver backfill\n",
    "for date_str in dates_str_lst:\n",
    "    utils.data_processing_silver_table.process_silver_table(date_str, bronze_lms_directory, silver_loan_daily_directory, spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d37d68da-e596-4c89-bdd3-2aef969e35ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m utils\u001b[38;5;241m.\u001b[39mdata_processing_silver_table\u001b[38;5;241m.\u001b[39mprocess_silver_table(date_str, bronze_lms_directory, silver_loan_daily_directory, spark)\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "utils.data_processing_silver_table.process_silver_table(date_str, bronze_lms_directory, silver_loan_daily_directory, spark).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9b5fe-a68c-4aa5-8c3e-8c0f59b654a6",
   "metadata": {},
   "source": [
    "## EDA on credit labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "608e1387-c2d4-4b41-b5aa-425ad6376e5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read all CSV files into a single DataFrame\u001b[39;00m\n\u001b[0;32m      8\u001b[0m files_list \u001b[38;5;241m=\u001b[39m [folder_path\u001b[38;5;241m+\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;241m*\u001b[39mfiles_list)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# filter only completed loans\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_start_date\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# set dpd label definition\n",
    "dpd = 30\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "folder_path = silver_loan_daily_directory\n",
    "\n",
    "# Read all CSV files into a single DataFrame\n",
    "files_list = [folder_path+os.path.basename(f) for f in glob.glob(os.path.join(folder_path, '*'))]\n",
    "df = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "\n",
    "# filter only completed loans\n",
    "df = df.filter(col(\"loan_start_date\") < datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\"))\n",
    "\n",
    "# create dpd flag if more than dpd\n",
    "df = df.withColumn(\"dpd_flag\", F.when(col(\"dpd\") >= dpd, 1).otherwise(0))\n",
    "\n",
    "# actual bads \n",
    "actual_bads_df = df.filter(col(\"installment_num\") == 10)\n",
    "\n",
    "# prepare for analysis\n",
    "# df = df.filter(col(\"installment_num\") < 10)\n",
    "\n",
    "# visualise bad rate\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Group by col_A and count occurrences in col_B\n",
    "grouped = pdf.groupby('mob')['dpd_flag'].mean()\n",
    "\n",
    "# Sort the index (x-axis) of the grouped DataFrame\n",
    "grouped = grouped.sort_index()\n",
    "\n",
    "# Plotting\n",
    "grouped.plot(kind='line', marker='o')\n",
    "\n",
    "plt.title('DPD: '+ str(dpd))\n",
    "plt.xlabel('mob')\n",
    "plt.ylabel('bad rate')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b200137d-ca20-42b9-a3df-0b662024a899",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a6b17-8978-405a-8b04-6c6f4c7a3e52",
   "metadata": {},
   "source": [
    "## Build gold table for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b4fca41-8b52-47e9-bf40-b063a0d7c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bronze datalake\n",
    "gold_label_store_directory = \"datamart/gold/label_store/\"\n",
    "\n",
    "if not os.path.exists(gold_label_store_directory):\n",
    "    os.makedirs(gold_label_store_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a7874eb-aa70-4cd4-81ca-1e955e788b44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run gold backfill\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date_str \u001b[38;5;129;01min\u001b[39;00m dates_str_lst:\n\u001b[1;32m----> 3\u001b[0m     utils\u001b[38;5;241m.\u001b[39mdata_processing_gold_table\u001b[38;5;241m.\u001b[39mprocess_labels_gold_table(date_str, silver_loan_daily_directory, gold_label_store_directory, spark, dpd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, mob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# run gold backfill\n",
    "for date_str in dates_str_lst:\n",
    "    utils.data_processing_gold_table.process_labels_gold_table(date_str, silver_loan_daily_directory, gold_label_store_directory, spark, dpd = 30, mob = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96439e0a-282e-4081-8865-ce9e2779757d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m utils\u001b[38;5;241m.\u001b[39mdata_processing_gold_table\u001b[38;5;241m.\u001b[39mprocess_labels_gold_table(date_str, silver_loan_daily_directory, gold_label_store_directory, spark, dpd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, mob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "utils.data_processing_gold_table.process_labels_gold_table(date_str, silver_loan_daily_directory, gold_label_store_directory, spark, dpd = 30, mob = 6).dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeb8a3-7737-4556-a85c-e844b47f6454",
   "metadata": {},
   "source": [
    "## inspect label store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0abc72e2-90ed-46da-8c6e-599573d54049",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m gold_label_store_directory\n\u001b[0;32m      2\u001b[0m files_list \u001b[38;5;241m=\u001b[39m [folder_path\u001b[38;5;241m+\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;241m*\u001b[39mfiles_list)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_count:\u001b[39m\u001b[38;5;124m\"\u001b[39m,df\u001b[38;5;241m.\u001b[39mcount())\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "folder_path = gold_label_store_directory\n",
    "files_list = [folder_path+os.path.basename(f) for f in glob.glob(os.path.join(folder_path, '*'))]\n",
    "df = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "print(\"row_count:\",df.count())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20833e-9653-4c4c-9ce0-0023cca719b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845dda2a-1659-407d-9e98-df1dbdb3f025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545f517-df45-4c0c-8801-55b335f4879d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90a30-68f0-4c91-98fb-a1c4eef37211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
